{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deukkyo/NewProject/blob/master/DQN_Test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define Hyperparameters\n",
        "learning_rate = 0.005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "\n",
        "\n",
        "# ReplayBuffer Class\n",
        "class ReplayBuffer():\n",
        "  def __init__(self):\n",
        "    self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "  def put(self,transition):\n",
        "    self.buffer.append(transition)\n",
        "\n",
        "  def sample(self, n):\n",
        "    mini_batch = random.sample(self.buffer, n)\n",
        "    s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "    for transition in mini_batch:\n",
        "      s, a, r, s_prime, done_mask = transition\n",
        "      s_lst.append(s)\n",
        "      a_lst.append([a])\n",
        "      r_lst.append([r])\n",
        "      s_prime_lst.append(s_prime)\n",
        "      done_mask_lst.append([done_mask])\n",
        "\n",
        "    return torch.tensor(s_lst, dtype-torch.float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst,dtype=torch.float), torch.tensor(done_mask_lst)\n",
        "\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Qnet,self).__init__()\n",
        "    self.fc1=nn.Linear(4,128)\n",
        "    self.fc2 = nn.Linear(128,128)\n",
        "    self.fc3 = nn.Linear(128,2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def sample_action(self, obs, epsilon):\n",
        "    out = self.forward(obs)\n",
        "    coin = random.random()\n",
        "    if coin < epsilon:\n",
        "      return random.randint(0, 1)\n",
        "    else:\n",
        "      return out.argmax().item()\n",
        "\n",
        "\n",
        "def train(q, q_target, memory, optimizer):\n",
        "  for i in range(10):\n",
        "    s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
        "\n",
        "    q_out = q(s)\n",
        "    q_a = q_out.gather(1,a)\n",
        "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
        "    target = r + gamma * max_q_prime * done_mask\n",
        "    loss = F.smooth_l1_loss(q_a, target)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "def main():\n",
        "  env = gym.make('CartPole-v1')\n",
        "\n",
        "  q = Qnet()\n",
        "  q_target = Qnet()\n",
        "  q_target.load_state_dict(q.state_dic())\n",
        "  memory = ReplayBuffer()\n",
        "\n",
        "  print_interval = 20\n",
        "  score = 0.0\n",
        "\n",
        "  optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "\n",
        "  for n_epi in range(10000):\n",
        "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200))\n",
        "    \n",
        "    s = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
        "      s_prime, r, done, info = env.step(a)\n",
        "      done_mask = 0.0 if done else 1.0\n",
        "      memory.put((s,a,r/100.0,s_prime,done_mask))\n",
        "      s = s_prime\n",
        "      \n",
        "      score += r\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    if memory.size()>2000:\n",
        "      train(q, q_target, memory, optimizer)\n",
        "\n",
        "    if n_epi%print_interval==0 and n_epi!= 0:\n",
        "      q_target.load_state_dict(q.state_dic())\n",
        "      print(\"n_episode :{}, score : {:.1f}, n_butter : {}, eps : {:.1f}%\"\n",
        "      .format(n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "\n",
        "      score = 0.0\n",
        "\n",
        "  env.close()"
      ],
      "metadata": {
        "id": "235hss6VcRgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DEBrTi4L15vo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colaboratory에 오신 것을 환영합니다의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}